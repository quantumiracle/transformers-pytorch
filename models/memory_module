from __future__ import annotations
from typing import Callable

from math import ceil
from copy import deepcopy
from functools import partial
from collections import namedtuple

import tqdm

import torch
from torch import nn, stack, cat
import torch.nn.functional as F
from torch.nn import Module, ModuleList, Linear
from einops import repeat, rearrange, pack, unpack
from einops.layers.torch import Rearrange
from rotary_embedding_torch import RotaryEmbedding

def round_up_multiple(seq, mult):
    return ceil(seq / mult) * mult

def pad_and_segment_with_inverse(
    seq,
    segment_len,
    fold_into_batch = True,
    inverse_remove_pad = True
):
    batch, seq_len = seq.shape[:2]
    next_seq_len_mult = round_up_multiple(seq_len, segment_len)

    padding = next_seq_len_mult - seq_len
    needs_pad = padding > 0

    if needs_pad:
        seq = F.pad(seq, (0, 0, 0, padding))

    if fold_into_batch:
        seq = rearrange(seq, 'b (w n) d -> (b w) n d', n = segment_len)

    def inverse(out):

        if fold_into_batch:
            out = rearrange(out, '(b w) ... n d -> b ... (w n) d', b = batch)

        if needs_pad and inverse_remove_pad:
            out = out[..., :-padding, :]

        return out

    return seq, inverse

def sigma_act(x):
    return F.elu(x) + 1


def updateMemory(kv_mem, k, v, z, delta=False):
    if kv_mem is not None:
        assert kv_mem.shape[1] == v.shape[1], "Memory length must be the same as the number of new keys"
    if delta and (kv_mem is not None):
        sigma_k = sigma_act(k)
        numerator = einsum(
            "b h N k, b h k v -> b h N v",
            sigma_k,
            kv_mem,
        )
        denominator = einsum(
            "b h N k, b h k -> b h N",
            sigma_k,
            z,
        )
        denominator = rearrange(
            denominator,
            "b h N -> b h N 1",
        )
        prev_v = numerator / denominator
        new_value_states = v - prev_v
        new_kv_mem = torch.matmul(sigma_k.transpose(-2, -1), new_value_states)
    else:
        k_T = rearrange(sigma_act(k), "B h N d -> B h d N")
        new_kv_mem = sigma_act(k_T) @ v
    if kv_mem is not None:
        new_kv_mem = kv_mem + new_kv_mem
    return new_kv_mem

def getAttnMem(kv_mem, z, q):
    assert kv_mem is not None, "Attention memory must be provided"
    sigma_q = sigma_act(q)
    retrieved_memory = einsum(
        "b h N k, b h k v -> b h N v",
        sigma_q,
        kv_mem,
    )

    denominator = einsum(
        "b h N d, b h d -> b h N",
        sigma_q,
        z,
    )
    denominator = rearrange(
        denominator,
        "b h N -> b h N 1",
    )
    retrieved_memory = retrieved_memory / denominator
    return retrieved_memory

def updateZ(z, k):
    # k: (B, h, N, d)
    k = sigma_act(k)
    new_z = torch.sum(k, dim=2, keepdim=False)
    if z is not None:
        new_z = z + new_z
    return new_z #

class InfiniAttention(nn.Module):
    """
    InfiniAttention is a variant of the Attention class that allows for fixed length of the cached keys and values as memory.

    """
    def __init__(
        self, 
        dim, 
        heads = 8, 
        dim_head = 64, 
        is_causal = True, 
        bptt = False, 
        delta_update = False,
        segment_len = 32,
    ):
        super().__init__()
        inner_dim = dim_head *  heads
        self.heads = heads
        self.scale = dim_head ** -0.5
        self.norm = nn.LayerNorm(dim)
        self.rotary_emb = RotaryEmbedding(dim_head)
        self.attend = nn.Softmax(dim = -1)

        self.is_causal = is_causal
        self.bptt = bptt
        self.delta_update = delta_update
        self.segment_len = segment_len
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)
        self.to_out = nn.Linear(inner_dim, dim, bias = False)

        # memory params
        self.mem_beta = nn.Parameter(torch.zeros(1, heads, 1, 1))

    def forward(self, x, cache = None, memory_cache = None):
        # auto pad to multiple
        x, inverse_segment = pad_and_segment_with_inverse(x, self.segment_len, fold_into_batch = False)

        x = self.norm(x)

        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)

        # caching
        if exists(cache):
            past_k, past_v = cache
            k = torch.cat((past_k, k), dim = -2)
            v = torch.cat((past_v, v), dim = -2)
        next_cache = tuple(map(inverse_segment, (k, v)))

        # relative positions
        q, k = self.rotary_emb.rotate_queries_with_cached_keys(q, k)

        # segment

        q, k, v = tuple(rearrange(t, 'b h (w n) d -> w b h n d', n = total_segment_len) for t in (q, k, v))

        # initialize memory and z
        mem = torch.zeros(1, self.heads, self.dim_head, self.dim_head)
        z = torch.zeros(1, self.heads, self.segment_len, 1)

        outputs = []
        for i in range(k.shape[0]):
            q_slice = q[i]
            k_slice = k[i]
            v_slice = v[i]

            # a simpler variant for attention computation
            out = F.scaled_dot_product_attention(query=q_slice, key=k_slice, value=v_slice, is_causal=self.is_causal)

            # memory retrieval
            mem_out = getAttnMem(mem, z, q_slice)
            out = F.sigmoid(self.mem_beta) * mem_out + (1 - F.sigmoid(self.mem_beta)) * out

            outputs.append(out)

            # memory update
            if self.bptt:
                new_mem = updateMemory(mem, k_slice, v_slice, z, self.delta_update)
                new_z = updateZ(z, k_slice)
            else:
                with torch.no_grad():
                    new_mem = updateMemory(mem, k_slice, v_slice, z, self.delta_update)
                    new_z = updateZ(z, k_slice)

        outputs = torch.concat(outputs)

        out = self.to_out(outputs)

        out = rearrange(out, '(w b) n d -> b (w n) d', b = batch)

        out = inverse_segment(out)
        out = rearrange(out, 'b h n d -> b n (h d)')

        return out, next_cache
